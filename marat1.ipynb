{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57e380ce-d538-4d08-89d3-1fbebc2470c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\"a\" : 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "145df1c9-81ac-483b-8f29-2e6f6411c8ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'c'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'c'"
     ]
    }
   ],
   "source": [
    "d[\"c\"] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "784245ab-75cf-4235-b75a-79c0f690e46b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "06f67721-4b31-4969-92bf-59e8dfcf39e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/jupyter-admin/.local/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: tqdm in /opt/tljh/user/lib/python3.9/site-packages (from nltk) (4.62.3)\n",
      "Requirement already satisfied: click in /opt/tljh/user/lib/python3.9/site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/jupyter-admin/.local/lib/python3.9/site-packages (from nltk) (2022.8.17)\n",
      "Requirement already satisfied: joblib in /home/jupyter-admin/.local/lib/python3.9/site-packages (from nltk) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68f20d99-45ec-4065-9685-d00dc5d2c184",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting natasha\n",
      "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
      "     |████████████████████████████████| 34.4 MB 1.1 MB/s            \n",
      "\u001b[?25hCollecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "     |████████████████████████████████| 55 kB 4.6 MB/s             \n",
      "\u001b[?25hCollecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting razdel>=0.5.0\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting slovnet>=0.3.0\n",
      "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
      "     |████████████████████████████████| 49 kB 7.7 MB/s             \n",
      "\u001b[?25hCollecting yargy>=0.14.0\n",
      "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
      "     |████████████████████████████████| 41 kB 157 kB/s             \n",
      "\u001b[?25hCollecting intervaltree>=3\n",
      "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /opt/tljh/user/lib/python3.9/site-packages (from navec>=0.9.0->natasha) (1.23.2)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "     |████████████████████████████████| 8.2 MB 31.0 MB/s            \n",
      "\u001b[?25hCollecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting sortedcontainers<3.0,>=2.0\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Building wheels for collected packages: docopt, intervaltree\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=9c0afb8755665d34f6e243358bd111eda2ed76d171b94a1bacac9e00956ef21d\n",
      "  Stored in directory: /home/jupyter-admin/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "  Building wheel for intervaltree (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26119 sha256=740eb027ac68b46a56e286855a166951b40dc02b1b2e75b7c214196c3d58ba88\n",
      "  Stored in directory: /home/jupyter-admin/.cache/pip/wheels/ab/fa/1b/75d9a713279796785711bd0bad8334aaace560c0bd28830c8c\n",
      "Successfully built docopt intervaltree\n",
      "Installing collected packages: sortedcontainers, pymorphy2-dicts-ru, docopt, dawg-python, razdel, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
      "\u001b[33m  WARNING: The script razdel-ctl is installed in '/home/jupyter-admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script pymorphy is installed in '/home/jupyter-admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "\u001b[33m  WARNING: The script navec-train is installed in '/home/jupyter-admin/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 sortedcontainers-2.4.0 yargy-0.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install natasha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "847bc231-923e-4fe0-9eba-98ef3cdadb99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pymorphy2 in /home/jupyter-admin/.local/lib/python3.9/site-packages (0.9.1)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /home/jupyter-admin/.local/lib/python3.9/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/jupyter-admin/.local/lib/python3.9/site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/jupyter-admin/.local/lib/python3.9/site-packages (from pymorphy2) (0.6.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c6fb5bf-ae5d-4638-a027-069518749422",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import natasha\n",
    "import pymorphy2\n",
    "from natasha import MorphVocab\n",
    "from natasha import (\n",
    "    Segmenter,\n",
    "    \n",
    "    NewsEmbedding,\n",
    "    NewsMorphTagger,\n",
    "    NewsSyntaxParser,\n",
    "    \n",
    "    Doc\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5be57fe-17aa-4254-a5c1-4607abe53cc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = SnowballStemmer(\"russian\") \n",
    "stemmer.stem(\"дом\") in stemmer.stem(\"домашний\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dbf5f8a-3c03-49d9-86cf-335e3d2141d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_x_train = pd.read_csv(\"x_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "caf6d3a0-2b82-4ad6-bba2-1cfe11cb2382",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = df_x_train[\"tokens\"].unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e7f014-3923-4eab-bac9-c0efe4aa5307",
   "metadata": {},
   "source": [
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d628f453-9ae8-4ec1-9d1a-a1f56a974b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=tokens[0].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc01687-65be-4da6-8a2a-5fd99905c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    try:\n",
    "        a = token.split(\" \")\n",
    "        for i in range(0, len(a)-2, 2):\n",
    "            root = stemmer.stem(a[i])\n",
    "            try:\n",
    "                words[root] += a[i + 1]\n",
    "            except:\n",
    "                words[root] = a[i + 1]\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "32424652-5cd1-4a72-a54c-09ddc63d662b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(0,10,2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f1b5e52-b3d1-4d48-9873-710e6452a460",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[*range(0,10)][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "869dc5d4-097f-47dd-ac46-5c161f423d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmenter = Segmenter()\n",
    "\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "morph_vocab = MorphVocab()\n",
    "\n",
    "doc = Doc(\"parasha\")\n",
    "doc.segment(segmenter)\n",
    "doc.tag_morph(morph_tagger)\n",
    "doc.parse_syntax(syntax_parser)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "38a62292-44e2-49bc-ab4c-741590ebd2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_vocab = MorphVocab()\n",
    "\n",
    "for token in doc.tokens:\n",
    "    token.lemmatize(morph_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b568d924-fcd8-4030-9cb9-bb7ccc76f3cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['parasha']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.lemma for i in doc.tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc382300-7add-4a51-8f82-7017006d5102",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "emb = NewsEmbedding()\n",
    "morph_tagger = NewsMorphTagger(emb)\n",
    "syntax_parser = NewsSyntaxParser(emb)\n",
    "\n",
    "for tok in tokens:\n",
    "    try:\n",
    "        b = tok[0].split(\" \")\n",
    "        a = b[::2]\n",
    "        a_len = len(a)\n",
    "        a_str = \" \".join(a)\n",
    "        doc = Doc(a_str)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.parse_syntax(syntax_parser)\n",
    "\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "        lemmas = [i.lemma for i in doc.tokens]\n",
    "        for i in range(0, a_len-1):\n",
    "                    try:\n",
    "                        words[lemmas[i]] += b[2 * i + 1]\n",
    "                    except:\n",
    "                        words[lemmas[i]] = b[2 * i + 1]\n",
    "    except:\n",
    "        pass\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "415bb139-17bb-41bd-afa6-7b05a9ab80c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \" \".join(tok[0].split(\" \")[::2])\n",
    "a\n",
    "words = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cba3da9f-69d2-4dcb-99c8-0d2d2f3548e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e77ff63-4692-4a08-ac9a-bd48242aa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "\n",
    "\n",
    "for t in tok[::10]:\n",
    "    try:\n",
    "        b = t.split(\" \")\n",
    "        a = b[::2]\n",
    "        a_len = len(a)\n",
    "        a_str = \" \".join(a)\n",
    "        doc = Doc(a_str)\n",
    "        doc.segment(segmenter)\n",
    "        doc.tag_morph(morph_tagger)\n",
    "        doc.parse_syntax(syntax_parser)\n",
    "\n",
    "        for token in doc.tokens:\n",
    "            token.lemmatize(morph_vocab)\n",
    "        lemmas = [i.lemma for i in doc.tokens]\n",
    "        for i in range(0, a_len-1):\n",
    "                    try:\n",
    "                        words[lemmas[i]] += b[2 * i + 1]\n",
    "                    except:\n",
    "                        words[lemmas[i]] = b[2 * i + 1]\n",
    "    except:\n",
    "        n+=1\n",
    "        pass\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4a0b2f21-c389-4aef-87ca-c5a355770565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "293101"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd459f3-925d-4092-b795-8b8b0c354245",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = {}\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "for t in tok:\n",
    "    try:\n",
    "        b = t.split(\" \")\n",
    "        a = [morph.parse(word)[0].normal_form for word in b[::2]]\n",
    "        a_len = len(a)\n",
    "        for i in range(0, a_len-1):\n",
    "            try:\n",
    "                words[a[i]] += int(b[2 * i + 1])\n",
    "            except:\n",
    "                words[a[i]] = int(b[2 * i + 1])\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bebcba-dbfb-4088-b12b-23eb341d0e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_words = pd.DataFrame(words.items())\n",
    "df_words.to_csv(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5213dd4d-2f97-4cbc-bc7b-49b13a0c1bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
